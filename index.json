[{"uri":"https://aaubs.github.io/ds22/m1/01_basics/01_stat_prog/","title":"Basics Statistical Programming","tags":[],"description":"","content":"\nThis session is a basic introduction to statistical programming as well as a short brush-up on data more generally. For some, this will be \u0026ldquo;old news\u0026rdquo;, but many will certainly benefit from reviewing this material. We start with a general theory lecture on data structures and properties, and then dive into Python specific applications of statistical programming.\nPart 1: Basics Notebook: Python 101 Part 2: Basic Data Manipulation Notebook: Python Data Manipulation Exercises Notebook: Python DS Handbook, C.2-3 exercises\nNotebook: Pandas exercises\nHere, you will find the answers to the exercises:\nNotebook: Python DS Handbook, C.2-3 exercises and solutions\nNotebook: Pandas exercises and solutions\nFurther studies Recommended DataCamp courses Introduction to Python Intermediate Python Recommended readings Python fo Data Science Handbook (VanderPlas, 2016), Chapter 2-3 Further resources "},{"uri":"https://aaubs.github.io/ds22/info/","title":"Info, Schedule &amp; Co","tags":[],"description":"","content":"General info about the semester will be updated here. Check out the calendar on moodle.\nIntro to the semester and module: "},{"uri":"https://aaubs.github.io/ds22/info/01_infrastructure/","title":"Infrastructure","tags":[],"description":"","content":"Main Infrastructure MS teams : Join our MS our teams channel to get updates from us, Q\u0026amp;A, and talk to your peers. Moodle : We will not use it too much, but for the sake of completeness. However, you will still find your calendar there and we may use it from time to time to send out mass-mails to you all. Datacamp : Get access to all the Datacamp premium content for free to facilitate your data science journey. Use your AAU mail here when signing up. Adittional Infrastructure used Github Provides internet hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, plus its own features. It is also commonly used to host open-source projects, including data science projects. If you do not have it already, you are advice to create an account to manage and showcase your work during this semester. Getting started Jupyter/Colab Intro Python Colab Github Hello World The Markdown Guide: Introduction to markdown, the formating language used in ipython notebooks. "},{"uri":"https://aaubs.github.io/ds22/m1/","title":"Applied Data Science and Machine Learning","tags":[],"description":"","content":"M1 - Applied Data Science and Machine Learning This module provides a condensed introduction to the “Data Science Pipeline”, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\n"},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/","title":"Basics of Statistical Programming and Data Manipulation (W35-36)","tags":[],"description":"","content":"This chapter is a basic introduction to statistical programming as well as a short brush-up on data more generally. In addition, we will have some statistics refreshers, reviewing important concepts. For some, this will be \u0026ldquo;old news\u0026rdquo;, but many will certainly benefit from reviewing this material. Afterwards, we introduces some fundamental concepts of data manipulation and exploratory data analysis (EDA).\nIntro slides Slides fullscreen "},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/02_intro_stats/","title":"Statistics Refresher","tags":[],"description":"","content":" Theory part is general, Python application part hands-on and language specific.\nThis session is a basic introduction and refresher of statistical concepts important to data science.\nPart 1: Statistics refresher Slides Notebookm statistics refresher Part 2: Further concepts Notebook propability distributions Notebook AB testing Further studies Recommended DataCamp courses Intyroduction to statistics (no coding) Statistical Thinking in Python I Statistical Thinking in Python II Statistical Simulation in Python Recommended readings Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python ###€ Further resources\nx "},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/03_data_visualization/","title":"Basics Data Visualization","tags":[],"description":"","content":"This session introduces some fundamental concepts of data visualization. After a theoretical lecture on types and dimensions of data visualization, we will explore the visualization of different data types, structure, and properties in R and Python specific applications.\nNotebooks Notebook Dataviz Slides Use arrows keys on keyboard to navigate. Alternatively fullscreen slides here\n"},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/03-intro-eda/","title":"Hands-on data manipulation and EDA ","tags":[],"description":"","content":" Corgi working on a Data Science project. 2022. Roman x Stable Diffusion\nIn this session we will be trying out some of the techniques already learned on real world data from AirBnb as well as experiment with Kaggle\nPart 1: AirBnb In this notebook we will be using data from AirBnb for some basic EDA and geoplotting\nEDA and Geoviz starter EDA and Geoviz class Part 2: Kaggle In this notebook we will be learning how to work with data from Kaggle as well as exercise more simple data-viz.\nKaggle starter Kaggle class What to do now?! Replay code from the course and see if you do understand the core mechanics - you DO NOT need to remember everything. Android app market project on datacamp Course: Python DS toolbox 1 \u0026amp; Course: Python DS toolbox 2 Opendata.dk - build a map of different attractions in Aalborg based on public data. See preprocessing example - how to get data out of nested JSON - below: This is how you can preprocess the GeoCoordinates from the JSON file:\n#Load pandas import pandas as pd # Read the file from remote data = pd.read_json(\u0026#39;https://admin.opendata.dk/dataset/44ecd686-5cb5-40f2-8e3f-b5e3607a55ef/resource/eeabb0f8-1b19-4c80-b059-5ba5c4c872d2/download/guidedenmarkaalborgenjson.json\u0026#39;) # The GeoCoordinates are hiding in the Address column data[\u0026#39;Address\u0026#39;][0][\u0026#39;GeoCoordinate\u0026#39;] # You can use list comprehension to pull out GeoCoordinates (also empty values) - try out # This will allow you to filter for missing data without fancy workarounds [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # Make a new column based on that to be used for filtering out missing data data[\u0026#39;GeoCoordinate\u0026#39;] = [x[\u0026#39;GeoCoordinate\u0026#39;] for x in data[\u0026#39;Address\u0026#39;]] # drop, where no GeoCoordinate data = data.dropna(subset=[\u0026#39;GeoCoordinate\u0026#39;]) # Pull out the values data[\u0026#39;latitude\u0026#39;] = [x[\u0026#39;Latitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] data[\u0026#39;longitude\u0026#39;] = [x[\u0026#39;Longitude\u0026#39;] for x in data[\u0026#39;GeoCoordinate\u0026#39;]] "},{"uri":"https://aaubs.github.io/ds22/info/02_modules/","title":"Modules","tags":[],"description":"","content":"For Business Data Science Students M1: Data Handling, Exploration \u0026amp; Applied Machine Learning 10 ECTS\nThis module will prove a condensed introduction to the “Data Science Pipeline”, introducing students to methods, techniques, and workflows in applied data analytics and machine learning, including data acquisition, preparation, analysis, visualization, and communication.\nM2: Network Analysis and Natural Language Processing 5 ECTS\nFocuses on analyzing a variety of unstructured data sources. Particularly, students will learn how to explore, analyze, and visualize natural language (text) as well as relational (network) data.\nM3: Data-Driven Business Modelling and Strategy 15 ECTS Course with integrated project in which you will learn how companies plan, prepare and execute data-driven projects. In the project you will work wich a company case and build a \u0026ldquo;mini\u0026rdquo; version of the product/process.\nFor Social Data Science Students - Elective Semester M3: (SDS) Deep Learning and Artificial Intelligence for Analytics 5 ECTS\nIntroduces to the most recent developments in machine learning, which are deep learning and artificial intelligence applications. The module will provide a solid foundation for this exciting and rapidly developing field. Students will learn whether and how to apply deep learning techniques for business analytics, and acquire proficiency in new methods autonomously.\nCapstone Project Semester project utilising techniques and approaches from SDS in the context of a problem related to your main study field.\n"},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/04-eda-open-policing/","title":"EDA workshop &amp; Working with JSON/APIs/MongoDB","tags":[],"description":"","content":"In this workshop we are going to explore 2 things. More advanced EDA as well as working with internet data.\nEDA - Open Policing Project In this workshop we are going to work with the Open Policing Dataset. Below you will find the starter notebook where this dataset (Rhode Island) is explored. You can check out some of the research based on this data ont the project’s page.\nColab Notebook\nInternet Data, APIs, JSON and MongoDB Digital nomads and corgis working at a beach cafe in Bali. 2022. Roman x Stable Diffusion\nIn this tutorial we will look into JSON, the probably most common datatype on the internet today and how we can work with it. For the tutorial you will either need to install a local copy of MongoDB or make a free-tier account with Atlas (MongoDB Cloud version).\nColab Notebook\nWhat to do now? Datacamp course: Intro to MongoDB "},{"uri":"https://aaubs.github.io/ds22/info/04_litetrature/","title":"Literature &amp; Resources","tags":[],"description":"","content":"While this course does not come with a list of mandatory readings, we will often refer to some central resources in R and python, which for the most part can always be accessed in a free and updated online version. We generally recommend you to use these amazing resources for problem-solving and further self-study on the topic.\nMain Literature These pieces of work can be seen as main references for data science using Python. We will frequently refer to selected chapters for further study. Documentation of the used packages, tutorials, papers, podcasts etc. will be added throughout.\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O\u0026rsquo;Reilly Media, Inc. Online available here Supplementary literature Essential Math for Data Science O\u0026rsquo;Reilly Media. Nield, T. (2022): Math refresher targeting data science relevant concepts. Econometrics with Python - Causal Inference for The Brave and True: More thorrough inferential statistics in Python Further Ressources Data Science Cloud services Notebook bases: Google Colab: Googles popular service for editing, running \u0026amp; sharing Jupyter notebooks (Only Python Kernel, but R kernel can be accessed via some tricks) Deepnote: New popular online notebook service with good integration to other services (Python, R \u0026amp; more) Kaggle: Also provides their own cloud-based service co create and run computational notebooks. Convenient, unlimited, but a bit slow (Pyhton, r ). Instance based: UCloud: New cloud infrastructure provided by AAU, AU, SDU AAU Strato: AAU CLAUDIA infratructure. Very powerful, but access needs a bit of experience with working via terminal. Community Kaggle: Crowdsourced data science challanges. Nowadays also provides a vivid community where you find datasets, notebooks for all kind of data science exercises. madewithml Tools \u0026amp; Helpers "},{"uri":"https://aaubs.github.io/ds22/info/03_schedule/","title":"Semester Schedule","tags":[],"description":"","content":"This will be shortly updated with additional key dates and topics for the semester. For now, please follow CalMoodle.\nGeneral appointments Introduction to Semester Project and group formation: 12.10.2022, 10:30-12:00 M1: Week 35-39 Topics W 35: Introduction \u0026amp; landing W 36: Data Manipulation, Exploratory Data Analysis (EDA) W 37: Exploratory Data Analysis (EDA) / Dashboard development / Hackathon W 38: Unsupervised Machine Learning (UML), Math for ML W 39: Supervised Machine Learning (SML) W 40: Group Assignment W 41: Exam Key Dates EDA Hackathon\nIn groups: Developing EDA Dashboard 9.09.2022 - 15.09.2022 Group assignment: 30.09.-05.10.2022 (Digital Eksamen)\nFinal exam: 10-11.09.2022\nM2: Week 40-44 Topics W 41: Introduction to Network Analysis (NW) W 42: Autumn break 🎉🍁 W 43: NW applications, Introduction to Natural-Language-Processing (NLP) W 44: Advanced applications in Network and Text Analysis / Module Assignment group work W 45: Exam Key Dates Group assignment: 31.10.-04.11.2022 (Digital Eksamen) Final exam: 9-10.11.2022 M3 / Semester-Project: Week 45-48 Topics W 41: Kick-off M3 course and semester project (12.10.2022, 10:30-12:00) W 44: Strategy and Business Modelling Workshop W 45: Project Management Workshop Key Dates Semester Project Submission: ~ 21/12. Exam: ~ 3-4 Week in January "},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/05-eda-hackathon/","title":"M1 EDA Hackathon ","tags":[],"description":"","content":" Team of corgis winning a hackathon. 2022. Roman x Stable Diffusion\nIn this hackathon, you will be using everything you have learned so far (and some things still coming up next week) to create a striking and informative dashboard. The winning group will receive an even more amazing prize!\nWhat to do? In groups, find an exciting dataset of similar complexity as the ones we worked with in class (e.g. comparable to Open Policing, Airbnb). You can find them, for instance, on Kaggle.\nConsider also:\nhttps://github.com/shreyashankar/datasets-for-good https://www.tableau.com/learn/articles/free-public-data-sets Steps for the analysis\nPerform EDA and answer interesting questions about the data. You can use geospatial analysis (if you like). You do not have to scrape or generate data, but you can. Perform analyses first in a notebook and generate all relevant calculations and plots there. On Sept. 13th, we\u0026rsquo;ll introduce you to Streamlit, a framework for webapp development. You are obviously welcome to look into it on your own before that. The submission deadline will be on the 15th (noon) - as a deployed (accessible online) web app. You will pitch your dashboard in class. We will forward all submitted apps to an expert jury for evaluation. The winning team will be announced in the week between 19. and 23. September. 🎉\nThe Jury Mathias Boe Flinta CTO at Scandinavian Medical Solutions\nMathias is Head of Data \u0026amp; Analytics \u0026amp; IT at SMS (Scandinavian Medical Solutions). He finished his studies in cand.oecon. (MSc. Economics) one year ago, during which he specialized in BI and Data Science. Currently, he is implementing a new ERP system, which will set a good foundation for BI analysis and predictive foresting with Data Science within the coming year.\nKarolina Grodzinska Data Analyst Co-op @ Schneider Electric, Boston US\nKarolina currently works as a data analyst at Schneider Electric. She\u0026rsquo;s also been chosen as a Tableau Student Ambassador for the upcoming academic year. In her free time, she likes to participate in data visualization challenges.\nDavid Jan Lazar Data Scientist at DataSentics, an Atos Company.\nDavid has completed a Social Data Science semester at Aalborg University and a Masters Degree from Business Intelligence at Aarhus University. He works as a data scientist at DataSentics, a consulting firm that provides business solutions based on machine learning in areas of business, finance, insurance, natural language processing, computer vision and more.\n"},{"uri":"https://aaubs.github.io/ds22/info/05_requirements_project/","title":"Semester Project Requirements","tags":[],"description":"","content":"Format Functional and self-contained notebook Happy to see GitHub repos (which you can use as your portfolio in the job market) Project report (30-ish pages - max. 45) Some study relation (but that is debatable and not necessarily required) Report is a (semi/non) technical documentation. Think about a corporate censor that you try to inform Content Problem formulation with some practical and theoretical motivation (no huge literature discussion) Methodology (not a critical realist vs positivist discussion but some ideas about what can be concluded potentially) Data sourcing and pre-processing strategy Overall architecture of the model(s) Modelling (incl. finetuning) Results Discussion / Conclusion Scope Uses different methods from the course (at least 2 modules) in a creative way Downloading data from kaggle/github and running an ML model is probably not enough for a good performance Creative combinations of methodologies, please: combine financial data with social media data to look at equity development extract information from text data and create networks. Use network indicators to supplement company data Evaluation will focus on correct application and communication of DS methods The level of \u0026ldquo;technicality\u0026rdquo; is as in the course with emphasis on application and intuition, not on ML engineering / mathematics However, you will need to demonstrate insight into statistics on a level that is required to discuss your assignment e.g. interpret and discuss performance indicators, outline strategies for improvement e.g. under/oversampling "},{"uri":"https://aaubs.github.io/ds22/m1/01_basics/06-rapid-prototyping-stramlit/","title":"Rapid Prototyping with Streamlit","tags":[],"description":"","content":"\nStreamlit was first released in October 2019 and has gained enormous popularity in the past year. The reason behind the framework\u0026rsquo;s success is the ease with which it allows data scientists to build data-driven web apps without the need to deal with frontend development or other dev-ops stuff while allowing them to incorporate all kinds of functions going far beyond just dashboards. Going from a Jupyter Notebook to a Streamlit app just requires adding a few lines of code and rewriting a few minor things.\nFrom ipynb to web-app In this tutorial, we will be going back to this Airbnb EDA Notebook from last week and building a little web-app from it.\nPlan of attack\nWe will isolate the data-prep and visualization parts we are interested in Build \u0026amp; test the app in uCloud Deploy the app via GitHub to the Streamlit Cloud (this step is optional, as you can also deploy via uCloud - ~2kr/day server costs) uCloud is a uninversity cloud service. You get 1000 DKK and 50GB storage to start with but you can apply for more. This is usually granted, as the service is not used a lot. It\u0026rsquo;s a great place to learn about modern platforms, infrastructure and more. You can play with different types of installations in a safe environment. You can also request very powerful machines.\nUCloud Set-up For this project you will need 2 app-containers running: Coder-python and Streamlit. Both can run with minimal CPU/RAM requirements. Streamlit can only run once you created a project in Code-python and saved an app.py file. It is a good idea, to create a public link and connect it to the streamlit-app. Thus, you can try out your app on your phone or share it. Saving changes in app.py will trigger imediate recomplies and your app will update everywhere.\nStreamlit syntax and layout Now, what do we need to turn our notebook into a web app?\n# 1. page-config st.set_page_config(page_title=\u0026#39;Streamlit - Dashboard 🤯\u0026#39;, page_icon=\u0026#34;🚀\u0026#34;, layout=\u0026#39;wide\u0026#39; ) # 2. Page layout - e.g. a title st.title(\u0026#34;AirBnb rentals in Copenhagen 🇩🇰\u0026#34;) Streamlit layout follows your script - things that come first, will be displayed first\u0026hellip;etc.\nLoading and preprocessing the data We can just proceed as in a notebook, but it is useful to rewrite the data loading and preprocessing into a function and add the @st.experimental_singleton decorator. Streamlit performs a re-run every time something is chaged (UI) by the user e.g. a new filter is set. To reduce processing time it\u0026rsquo;s a good idea not to re-run data-loading every single time.\n# LOAD DATA ONLY ONCE @st.experimental_singleton def load_data(): data = pd.read_csv(\u0026#39;http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/visualisations/listings.csv\u0026#39;) # also preprocess as we did in the notebook data = data[data.number_of_reviews \u0026gt; 0] data = data[data.room_type.isin([\u0026#39;Private room\u0026#39;, \u0026#39;Entire home/apt\u0026#39;])] data[\u0026#39;price_z\u0026#39;] = (data[\u0026#39;price\u0026#39;] - data[\u0026#39;price\u0026#39;].mean())/data[\u0026#39;price\u0026#39;].std(ddof=0) data[\u0026#39;price_z\u0026#39;] = data[\u0026#39;price_z\u0026#39;].abs() data = data[data.price_z \u0026lt; 3] data[\u0026#39;log_price\u0026#39;] = np.log(data[\u0026#39;price\u0026#39;]) return data # LOAD THE DATA NOW! data = load_data() The plots to be rendered We will go for 2 plots. A geo-visualization using pydeck and a simple altair bar plot to show prices in different areas of town.\nGeoplot\nlayer = pdk.Layer( \u0026#34;ScatterplotLayer\u0026#34;, data=data[[\u0026#39;name\u0026#39;,\u0026#39;room_type\u0026#39;,\u0026#39;price\u0026#39;, \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;]].dropna(), pickable=True, opacity=0.7, stroked=True, filled=True, radius_scale=10, radius_min_pixels=1, radius_max_pixels=100, line_width_min_pixels=1, get_position=[\u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;], get_radius=10*\u0026#34;log_price\u0026#34;, get_color=[255, 140, 0], get_line_color=[0, 0, 0], ) # Set the viewport location view_state = pdk.ViewState(latitude=data[\u0026#39;latitude\u0026#39;].mean(), longitude=data[\u0026#39;longitude\u0026#39;].mean(), zoom=12, pitch=50) # Renders r = pdk.Deck(layers=[layer], initial_view_state=view_state, #map_style=\u0026#39;mapbox://styles/mapbox/light-v9\u0026#39;, tooltip={\u0026#34;text\u0026#34;: \u0026#34;{name}\\n{room_type}\\n{price}\u0026#34;} ) Altair barplot\nWhen using altair, we need to add one more thing, which is the number of processed observations. Altair doesn\u0026rsquo;t want to process beyond 5000 observations. That means that you have two options. Either you limit your data, as we will do here, or you pre-computed things in e.g. pandas. Here we will use 2 simple if-statements to make sure that data is always max 5000 observations.\nif len(data) \u0026gt; 5000: data_alt = data.sample(5000) if len(data) \u0026lt;= 5000: data_alt = data We use altair to create a bar chart and let it calculate the mean of the price variable (x-axis) with y and colours being split by the room type. We spread the chart across rows that represent neighbourhoods. Also, we add a tooltip that displays the values for the individual bars. We set strokeWidth to 0 to make things a bit tidier.\nprice_chart = alt.Chart(data).mark_bar().encode( x=\u0026#39;mean(price):Q\u0026#39;, y=alt.Y(\u0026#39;room_type:O\u0026#39;,axis=alt.Axis(labels=False), title=\u0026#34; \u0026#34;), color=alt.Color(\u0026#39;room_type:N\u0026#39;, scale=alt.Scale(scheme=\u0026#39;lightorange\u0026#39;)), row=\u0026#39;neighbourhood:N\u0026#39;, tooltip=[\u0026#34;neighbourhood:N\u0026#34;, \u0026#34;mean(price):Q\u0026#34;] ).configure_view(strokeWidth=0).interactive() Introductin UI / Filters We\u0026rsquo;ll introduce 2 filters in the main page (you could also move them to the sidebar): price range and neighbourhood. As you can see below, the st.sliderand st.multiselect produce python objects (tuple and list) that we can use to filter our data DataFrame with plain Pandas. e.g. When only 3 neighbourhoods are selected neighbourhood_select will turn a list of 3 elements and data[data.neighbourhood.isin(neighbourhood_select)] will result in a dataframe where only listings from these areas are present.\n#filter for price-range price_selected = st.slider(\u0026#34;Select price range\u0026#34;, min_value = int(data.price.min()), max_value= int(data.price.max()), value = (300,3000), step=50) data = data[(data.price \u0026gt; price_selected[0]) \u0026amp; (data.price \u0026lt; price_selected[1])] #filter for neighborhoods neighbourhood_select = st.multiselect(\u0026#39;Select neighbourhoods\u0026#39;, data.neighbourhood.unique(), data.neighbourhood.unique()) data = data[data.neighbourhood.isin(neighbourhood_select)] Rendering the visualizations So far nothing is displayed. We only created the objects r(pydeck map) and price_chart altair chart. It\u0026rsquo;s up to you how you like to handle it. I find it a bit easier to separate compute and render parts. Here we are going to add a horizontal column split (to make things a bit more pretty). We split the screen into 5 parts, where the first gets 3 and the second 2. To display our chats, we use the streamlit functions st.pydeck_chart and st.altair_chart. You will find many more options in the streamlit documentation.\nrow1_1, row1_2 = st.columns((3, 2)) with row1_1: st.pydeck_chart(r) with row1_2: st.altair_chart(price_chart, use_container_width=False) Requirements Our imports for that project look like this:\nimport streamlit as st import streamlit.components.v1 as components import pydeck as pdk import numpy as np import pandas as pd import altair as alt alt.renderers.set_embed_options(theme=\u0026#39;dark\u0026#39;) In contrast to a notebook, we cannot really install packages on the fly. Considering a simple python deployment environment, we need to specify what packages (aside from streamlit) need to be installed so that the app can run. This is often done by adding a requirements.txt file to the project folder. We add all libraries that we loaded just to be sure. This is not always the best idea, as things can clash\u0026hellip; but that is a chapter of its own to be covered at a later point. 😵\npydeck altair numpy pandas All code in one place requirements.txt Expand to see code... pydeck numpy pandas altair app.py Expand to see code... #imports import streamlit as st import streamlit.components.v1 as components import pydeck as pdk import numpy as np import pandas as pd import altair as alt alt.renderers.set_embed_options(theme=\u0026#39;dark\u0026#39;) # page config st.set_page_config(page_title=\u0026#39;Streamlit - Dashboard 🤯\u0026#39;, page_icon=\u0026#34;🚀\u0026#34;, layout=\u0026#39;wide\u0026#39; ) #load data @st.experimental_singleton def load_data(): data = pd.read_csv(\u0026#39;http://data.insideairbnb.com/denmark/hovedstaden/copenhagen/2022-06-24/visualisations/listings.csv\u0026#39;) data = data[data.number_of_reviews \u0026gt; 0] data = data[data.room_type.isin([\u0026#39;Private room\u0026#39;, \u0026#39;Entire home/apt\u0026#39;])] data[\u0026#39;price_z\u0026#39;] = (data[\u0026#39;price\u0026#39;] -data[\u0026#39;price\u0026#39;].mean())/data[\u0026#39;price\u0026#39;].std(ddof=0) data[\u0026#39;price_z\u0026#39;] = data[\u0026#39;price_z\u0026#39;].abs() data = data[data.price_z \u0026lt; 3] return data data = load_data() # 2. Page layout - e.g. a title st.title(\u0026#34;AirBnb rentals in Copenhagen 🇩🇰\u0026#34;) #filter for price-range price_selected = st.slider(\u0026#34;Select price range\u0026#34;, min_value = int(data.price.min()), max_value= int(data.price.max()), value = (300,3000), step=50) data = data[(data.price \u0026gt; price_selected[0]) \u0026amp; (data.price \u0026lt; price_selected[1])] #filter for neighborhoods neighbourhood_select = st.multiselect(\u0026#39;Select neighbourhoods\u0026#39;, data.neighbourhood.unique(), data.neighbourhood.unique()) data = data[data.neighbourhood.isin(neighbourhood_select)] #geoplot layer = pdk.Layer( \u0026#34;ScatterplotLayer\u0026#34;, data=data[[\u0026#39;name\u0026#39;,\u0026#39;room_type\u0026#39;,\u0026#39;price\u0026#39;, \u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;]].dropna(), pickable=True, opacity=0.7, stroked=True, filled=True, radius_scale=10, radius_min_pixels=1, radius_max_pixels=100, line_width_min_pixels=1, get_position=[\u0026#34;longitude\u0026#34;, \u0026#34;latitude\u0026#34;], get_radius=10*\u0026#34;log_price\u0026#34;, get_color=[255, 140, 0], get_line_color=[0, 0, 0], ) # Set the viewport location view_state = pdk.ViewState(latitude=data[\u0026#39;latitude\u0026#39;].mean(), longitude=data[\u0026#39;longitude\u0026#39;].mean(), zoom=12, pitch=50) # Renders r = pdk.Deck(layers=[layer], initial_view_state=view_state, #map_style=\u0026#39;mapbox://styles/mapbox/light-v9\u0026#39;, tooltip={\u0026#34;text\u0026#34;: \u0026#34;{name}\\n{room_type}\\n{price}\u0026#34;} ) # prefilter for altair if len(data) \u0026gt; 5000: data_alt = data.sample(5000) if len(data) \u0026lt;= 5000: data_alt = data #altair plot price_chart = alt.Chart(data).mark_bar().encode( x=\u0026#39;mean(price):Q\u0026#39;, y=alt.Y(\u0026#39;room_type:O\u0026#39;,axis=alt.Axis(labels=False), title=\u0026#34; \u0026#34;), color=alt.Color(\u0026#39;room_type:N\u0026#39;, scale=alt.Scale(scheme=\u0026#39;lightorange\u0026#39;)), row=\u0026#39;neighbourhood:N\u0026#39;, tooltip=[\u0026#34;neighbourhood:N\u0026#34;, \u0026#34;mean(price):Q\u0026#34;] ).configure_view(strokeWidth=0).interactive() # display and layout row1_1, row1_2 = st.columns((3, 2)) with row1_1: st.pydeck_chart(r) with row1_2: st.altair_chart(price_chart, use_container_width=False) "},{"uri":"https://aaubs.github.io/ds22/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://aaubs.github.io/ds22/","title":"Social / Business Data Science 2022","tags":[],"description":"","content":"Social \u0026amp; Business Data Science 2022 Aalborg University Business School The corresponding Aalborg University Moodle course page can be found here. Note that for updated content this page rather than Moodle will be used. At AAUBS Data Science we believe in the power of open science and open education. Following AAU’s “Knowledge for the world” strategy, we aim at making our material available outside password protected university systems.\n"},{"uri":"https://aaubs.github.io/ds22/tags/","title":"Tags","tags":[],"description":"","content":""}]