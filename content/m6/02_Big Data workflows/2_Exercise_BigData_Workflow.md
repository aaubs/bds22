---
title: "Exercise 2: Performing a Big Data workflow with Spark and Polars"
weight: 2
disableToc: true
draft: false
---

<img src="https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.fiverr.com%2Fahmad1024%2Fdevelop-an-etl-pipeline-using-spark-hadoop-ecosystem&psig=AOvVaw3Xh-oxmJc33bOdg_DaSzzA&ust=1680058523254000&source=images&cd=vfe&ved=0CBAQjRxqFwoTCMCm7e3P_f0CFQAAAAAdAAAAABAE" width="20">

In this session, we will demonstrate how to use Apache Spark, a powerful Big Data processing engine, for processing large datasets in ML projects. The session will cover tasks such as setting up a Spark environment, reading and writing data, and performing transformations and aggregations on data. Additionally, we will also introduce Polars, a similar data manipulation library for Rust, and compare its features to those of Spark.

This session will provide hands-on exercises to reinforce your understanding and skills in working with Spark and Polars for processing big data in ML projects.





## Notebooks

* [Performing a Big Data workflow with Pandas and Polars]()
* [Performing a Big Data workflow with Pandas and Polars]()
* [Performing a Big Data workflow with Pandas and Polars]()
* [Performing a Big Data workflow with Pandas and Polars]()




